
```{r}
library(tidyverse)
library(tokenizers)
library(tidyverse)
library(tidytext)
library(SnowballC)  # Stemming
library(lsa)  # Stopwörter
library(pdftools)
library(future)
library(furrr)
library(widyr)
```

# Daten laden

```{r}
sentiws <- read_csv("https://osf.io/x89wq/?action=download")
```


```{r}
d_wahl <- pdf_text("/Users/chrissi/Documents/Studium/Module/Data Science/DS 2/Regierungsprogramm_Grüne_23.pdf") %>% 
  as_tibble()
```


Wörter zählen: Wie lang ist das Parteiprogramm?

```{r}
str_count(d_wahl$value, pattern = "\\w") %>% sum()
```

```{r}
stop1 <- tibble(word = quanteda::stopwords("german"))
```


```{r}
d_wahl_token <- 
  d_wahl %>% 
  unnest_tokens(output = word, input = value) %>% 
  anti_join(stop1) %>% 
  count(word, sort = TRUE) 
```


Zahlen entfernen:

```{r}
d_wahl_token %>% 
  dplyr::filter(str_detect(word, "[A-Za-z]")) -> gruene_long
```


# Stemming

```{r}
gruene_stem <- 
  gruene_long %>% 
  mutate(stem = wordStem(word, language = "german")) %>% 
  count(stem, sort = TRUE)
```

```{r}
gruene_stem
```



# Sentiment-Analyse


```{r}
senti2 <- sentiws %>% 
  unnest_tokens(output = inflections_tidy, input = inflections) 
```

Jetzt sind die inflections aufgeräumter und untereinander. Praktischerweise wurde value immer übernommen.
Jetzt hänge ich inflections_tidy unten an words ran


```{r}
senti3 <- data.frame(word = c(sentiws$word, senti2$inflections_tidy),
                     value = c(sentiws$value, senti2$value),
                     neg_pos = c(sentiws$neg_pos, senti2$neg_pos))
```

Jetzt haben wir alle Spalten ergänzt und somit mehr Wörter für eine Sentiment-Analyse.

Problem: Manche Wörter doppelt. z.B. abbauen mit zwei unterschiedlichen Werten -> Dopplungen entfernen?


```{r}
senti4 <- senti3 %>% 
  distinct(word, .keep_all = TRUE)
```

Alle doppelten Werte entfernen, hierbei die Werte aus dem ursprünglichen df alle behalten <3

```{r}
gruene_senti <- 
  gruene_long %>% 
  inner_join(senti4, by = c("word" = "word"))  #nur die Werte behalten, die im Lexikon drin sind und danach zählen

```


```{r}
gruene_senti %>% 
  slice(1:15) %>% 
  ggplot()+
  aes(y = reorder(word, n), x = n)+
  geom_col()+
  geom_col(fill = "purple")
```


```{r}
gruene_senti_tab <- 
  gruene_senti %>% 
  group_by(neg_pos) %>% 
  summarise(polarity_mean = mean(value),
            polarity_sum = sum(value),
            polarity_count = n()) %>% 
  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) 
```


```{r}
show(gruene_senti_tab)
```

## Negativste Wörter

```{r}
gruene_senti %>% 
  dplyr::filter(neg_pos == "neg") %>% 
  mutate(value_abs = abs(value)) %>% 
  top_n(20, value_abs) %>% 
  pull(word)
```

## Positivste Wörter

```{r}
gruene_senti %>% 
  dplyr::filter(neg_pos == "pos") %>% 
  mutate(value_abs = abs(value)) %>% 
  top_n(20, value_abs) %>% 
  pull(word)
```


# Word embedding


```{r}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, ~ .x,
    .after = window_size - 1,
    .step = 1,
    .complete = FALSE
  )
  
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams, 1:length(skipgrams), ~ safe_mutate(.x, window_id = .y))
  
  out %>% 
    transpose() %>% 
    pluck("result") %>% 
    compact() %>% 
    bind_rows()
  
}
```


## Ähnlichkeit berechnen

```{r}
plan(multisession)
```


```{r}
nested_words <- gruene_long %>% 
  nest(words = c(word)) %>% 
  mutate(id = 1:length(words))
```


```{r}
slide_windows(nested_words, 4) -> skips
```

```{r}
tidy_pmi <- 
  skips %>% 
  unnest(words) %>% 
  pairwise_pmi(word, window_id)
```


```{r}
head(tidy_pmi)
```


## Nearest neighbours
Aus smltar:

```{r}
tidy_word_vectors <- tidy_pmi %>%
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )

tidy_word_vectors
```




```{r}
nearest_neighbours <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE
    )(item1, dimension, value) %>%
    select(-item2)
}
```


```{r}
tidy_word_vectors %>% 
  nearest_neighbours("bayern")
```

```{r}
tidy_word_vectors %>% 
  nearest_neighbours("verbieten")
```
